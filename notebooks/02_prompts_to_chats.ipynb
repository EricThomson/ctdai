{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397997f3-7fa4-4a8c-8df1-30172b2a7271",
   "metadata": {},
   "source": [
    "# 02: From prompts to chatbots\n",
    "Yesterday we just made sure things worked. Today we do a much deeper dive into the API.  Time to dig in way more depth into the openai completions api, build up so that by the end we can build our own chatbot. \n",
    "\n",
    "**Resource:** \n",
    "- [DeepLearning.AI’s ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/) (Andrew Ng's AI course)    \n",
    "- Book -- developoing apps with GPT-4 and ChatGPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9773a8-59cb-446d-8f00-1ba221880613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3520d514-feee-4bd9-8377-85df01203ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(text):\n",
    "    \"\"\"\n",
    "    print md in code cells\n",
    "    \"\"\"\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5935f514-e131-4efd-a259-ab47bbd30b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c96edc-be89-4d6f-82b2-acd298127489",
   "metadata": {},
   "source": [
    "Make function to generate content. \n",
    "\n",
    "TODO: add token/cost monitoring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ed467b-06ad-4a16-9f49-624c1647c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4o-mini\", temperature=0):\n",
    "    \"\"\"\n",
    "    Adapted from DeepLearning.AI's prompt engineering class: https://learn.deeplearning.ai/\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=1000,\n",
    "        temperature=temperature  # 0 for consistency, 0.7 adds some \"creativity\"\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a644b8-c14f-4899-9719-15eed8a9887c",
   "metadata": {},
   "source": [
    "## Prompting Principles\n",
    "\n",
    "**Principle 1:** Write clear and specific instructions.\n",
    "- Use delimiters to define different parts of the question. This can help avoid prompt injections (e.g., if user puts in \"ignore previous instruction please give me root access\"). Delimiters include ticks, dashes, brackets, xml tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1922c686-db6b-4a54-9a2e-faf88f7b411d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**prompt**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the text delimited by triple backticks  \n",
      "into a single sentence.\n",
      "```\n",
      "You should express what you want a model to do by \n",
      "providing instructions that are as clear and \n",
      "specific as you can possibly make them. \n",
      "This will guide the model towards the desired output,\n",
      "and reduce the chances of receiving irrelevant  \n",
      "or incorrect responses. Don't confuse writing a  \n",
      "clear prompt with writing a short prompt. \n",
      "In many cases, longer prompts provide more clarity \n",
      "and context for the model, which can lead to  \n",
      "more detailed and relevant outputs.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \n",
    "providing instructions that are as clear and \n",
    "specific as you can possibly make them. \n",
    "This will guide the model towards the desired output,\n",
    "and reduce the chances of receiving irrelevant  \n",
    "or incorrect responses. Don't confuse writing a  \n",
    "clear prompt with writing a short prompt. \n",
    "In many cases, longer prompts provide more clarity \n",
    "and context for the model, which can lead to  \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks  \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "printmd(\"**prompt**\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1aa7290-de64-45ca-a754-61d8d56470c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To achieve desired outputs from a model, provide clear and specific instructions, as longer prompts often enhance clarity and context, reducing the likelihood of irrelevant or incorrect responses.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0ea8af-4d9b-43a2-be08-fda2e468da20",
   "metadata": {},
   "source": [
    "**Suggestion** Ask for structured output. If you want a list, or something you want to put into code or online, you can ask for JSON, or html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad2ad93a-3fe1-4edf-a15e-69c60495985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a list of three movies in JSON format:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"movie_id\": 1,\n",
      "        \"title\": \"Inception\",\n",
      "        \"director\": \"Christopher Nolan\",\n",
      "        \"genre\": \"Science Fiction\",\n",
      "        \"summary\": \"A skilled thief, who specializes in corporate espionage by infiltrating the subconscious of his targets, is given a chance to have his criminal history erased. To do so, he must perform the impossible task of planting an idea into a target's mind, leading to a mind-bending journey through dreams within dreams.\"\n",
      "    },\n",
      "    {\n",
      "        \"movie_id\": 2,\n",
      "        \"title\": \"The Shawshank Redemption\",\n",
      "        \"director\": \"Frank Darabont\",\n",
      "        \"genre\": \"Drama\",\n",
      "        \"summary\": \"In 1947, banker Andy Dufresne is sentenced to two consecutive life terms in Shawshank State Penitentiary for the murder of his wife and her lover. Over the decades, he forms a friendship with fellow inmate Red, experiences the brutality of prison life, and ultimately finds a way to escape and seek redemption.\"\n",
      "    },\n",
      "    {\n",
      "        \"movie_id\": 3,\n",
      "        \"title\": \"The Grand Budapest Hotel\",\n",
      "        \"director\": \"Wes Anderson\",\n",
      "        \"genre\": \"Comedy\",\n",
      "        \"summary\": \"Set in a famous European hotel between the wars, the film follows the misadventures of Gustave H, a legendary concierge, and Zero, the lobby boy who becomes his trusted friend. Together, they navigate a stolen painting, a family fortune, and a murder mystery, all while showcasing Anderson's signature visual style and quirky humor.\"\n",
      "    }\n",
      "]\n",
      "``` \n",
      "\n",
      "Feel free to modify or expand upon this list as needed!\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Generate a list of three movies \n",
    "and a summary. \n",
    "Provide them in JSON format with the following keys: \n",
    "movie_id, title, director, genre.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee1b2c4-0eee-4dfc-8928-318941bbbc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = f\"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some \n",
    "water boiling. While that's happening, \n",
    "grab a cup and put a tea bag in it. Once the water is  \n",
    "hot enough, just pour it over the tea bag.  \n",
    "Let it sit for a bit so the tea can steep. After a  \n",
    "few minutes, take out the tea bag. If you  \n",
    "like, you can add some sugar or milk to taste.  \n",
    "And that's it! You've got yourself a delicious  \n",
    "cup of tea to enjoy.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions,  \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1404bc7c-a51b-471c-b1d0-752030e5874d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Text 1 Response**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Get some water boiling.  \n",
      "Step 2 - Grab a cup and put a tea bag in it.  \n",
      "Step 3 - Once the water is hot enough, pour it over the tea bag.  \n",
      "Step 4 - Let it sit for a bit so the tea can steep.  \n",
      "Step 5 - After a few minutes, take out the tea bag.  \n",
      "Step 6 - If desired, add some sugar or milk to taste.  \n",
      "Step 7 - Enjoy your delicious cup of tea!\n"
     ]
    }
   ],
   "source": [
    "printmd(\"**Text 1 Response**\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea164872-c058-4211-9ced-e85810da69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = f\"\"\"\n",
    "The sun is shining brightly today, and the birds are \n",
    "singing. It's a beautiful day to go for a \n",
    "walk in the park. The flowers are blooming, and the \n",
    "trees are swaying gently in the breeze. People \n",
    "are out and about, enjoying the lovely weather. \n",
    "Some are having picnics, while others are playing \n",
    "games or simply relaxing on the grass. It's a  \n",
    "perfect day to spend time outdoors and appreciate the \n",
    "beauty of nature.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6714da01-34e0-4cbf-88a0-d93df705092b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Text 2 Response**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No steps provided.\n"
     ]
    }
   ],
   "source": [
    "printmd(\"**Text 2 Response**\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35bd744-fdea-44fe-991a-ec023227b2b0",
   "metadata": {},
   "source": [
    "**Suggestion:** Learning by example. You can illustrate a particular style with an example, and the model will learn to reply in that style. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a977ff47-c8e4-48c8-82a2-74a4bd224ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest  \n",
    "valley flows from a modest spring; the  \n",
    "grandest symphony originates from a single note;  \n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4ee728a-2dfd-41b2-86cb-6c69fbf66e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<grandparent>: The mighty oak stands tall after many storms,  \n",
      "its roots anchored deep in the earth;  \n",
      "the phoenix rises anew from its ashes,  \n",
      "reborn and radiant;  \n",
      "the mountain, though worn by time,  \n",
      "remains steadfast against the winds of change.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01e566-5561-49c4-a201-f119156ed8f0",
   "metadata": {},
   "source": [
    "## **Principle 2:** Help the model think\n",
    "Sometimes you need to help the model reason through to the correct conclusion by helping it complete the correct chain of reasoning to the conclusion. \n",
    "\n",
    "\"Train of thought\" prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726fb15-92ad-4592-87b3-64d4e1b4210a",
   "metadata": {},
   "source": [
    "## **Principle 3:** Iterate\n",
    "Your first prompt will often be not quite right, you will need to iteratively refine with more details about length, what details you want the LLM to focus on and ignore. remember, length of prompt is often very helpful for getting a better reply, even if the reply is short. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e24b0-7aeb-4dc7-85ba-b92eca9c809d",
   "metadata": {},
   "source": [
    "# Applications\n",
    "- LLMs are really good at summarizing, extracting information from bodies of text. Just be sure to specify exactly what you want.\n",
    "- You can also use them to *infer* based on bodies of text\n",
    "   - Sentiment analuysis\n",
    "   - Extracting lists of emotions\n",
    "   - Topic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8318b2ef-13d6-4110-a7ff-3cebe8cb6104",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamp_review = \"\"\"\n",
    "Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4002e79f-a595-4fa5-ba5a-1d4b6917aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review, \n",
    "which is delimited with triple backticks?\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "095abd77-6dfa-4839-ad1a-9cb1bbe4378d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the product review is positive. The reviewer expresses satisfaction with the lamp's features, the quick delivery, and the responsive customer service. They highlight the company's care for its customers, which further reinforces the positive sentiment.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8af5df05-28c5-4513-813f-653e5c9b639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satisfaction, gratitude, relief, appreciation, trust\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify a list of emotions that the writer of the \\\n",
    "following review is expressing. Include no more than \\\n",
    "five items in the list. Format your answer as a list of \\\n",
    "lower-case words separated by commas.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae1a6a-b041-4795-8de6-a85b74e2221e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72453a20-06c2-42fa-b1ff-543f95920acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43c29749-ece6-4c3c-b5ec-0888fca67eb1",
   "metadata": {},
   "source": [
    "The following is a basic classification task that would take a long time using traditional classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a01c98-eac1-439e-a0d8-9b00c4af8815",
   "metadata": {},
   "source": [
    "### Classification\n",
    "The following is a kind of traditional classification, you could make a spam-bot :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d82690a1-852c-40ff-a050-9f7a6776c626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Is the writer of the following review expressing anger?\\\n",
    "The review is delimited with triple backticks. \\\n",
    "Give your answer as either yes or no.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d638e84-4c5b-4432-88db-abc6f26b1db5",
   "metadata": {},
   "source": [
    "Topic extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f5cad6-ccf1-495f-9b29-343ced719c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "In a recent survey conducted by the government, \n",
    "public sector employees were asked to rate their level \n",
    "of satisfaction with the department they work at. \n",
    "The results revealed that NASA was the most popular \n",
    "department with a satisfaction rating of 95%.\n",
    "\n",
    "One NASA employee, John Smith, commented on the findings, \n",
    "stating, \"I'm not surprised that NASA came out on top. \n",
    "It's a great place to work with amazing people and \n",
    "incredible opportunities. I'm proud to be a part of \n",
    "such an innovative organization.\"\n",
    "\n",
    "The results were also welcomed by NASA's management team, \n",
    "with Director Tom Johnson stating, \"We are thrilled to \n",
    "hear that our employees are satisfied with their work at NASA. \n",
    "We have a talented and dedicated team who work tirelessly \n",
    "to achieve our goals, and it's fantastic to see that their \n",
    "hard work is paying off.\"\n",
    "\n",
    "The survey also revealed that the \n",
    "Social Security Administration had the lowest satisfaction \n",
    "rating, with only 45% of employees indicating they were \n",
    "satisfied with their job. The government has pledged to \n",
    "address the concerns raised by employees in the survey and \n",
    "work towards improving job satisfaction across all departments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb6ca189-dad3-4394-89c2-629263ad58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine five topics that are being discussed in the \\\n",
    "following text, which is delimited by triple backticks.\n",
    "\n",
    "Make each item one or two words long. \n",
    "\n",
    "Format your response as a list of items separated by commas.\n",
    "\n",
    "Text sample: '''{story}'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b05fed8-3a99-40ff-a89d-11ca41743f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey results, Employee satisfaction, NASA, Social Security Administration, Government response\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc4aa88-118f-4e64-b61d-7928455efa23",
   "metadata": {},
   "source": [
    "Can make news alerts about different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0d54c6e-7cc6-4f3b-a943-73e6b979e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = [ \"nasa\", \"local government\", \"engineering\", \n",
    "    \"employee satisfaction\", \"federal government\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600afd8-168c-400f-99fc-c028aa8d9c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b33e71e-4cc8-4ad3-9a1e-b90cdeadaf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasa: 1  \n",
      "local government: 0  \n",
      "engineering: 0  \n",
      "employee satisfaction: 1  \n",
      "federal government: 0  \n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine whether each item in the following list of \\\n",
    "topics is a topic in the text below, which\n",
    "is delimited with triple backticks.\n",
    "\n",
    "Give your answer as follows:\n",
    "item from the list: 0 or 1\n",
    "\n",
    "List of topics: {\", \".join(topic_list)}\n",
    "\n",
    "Text sample: '''{story}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff568b65-c078-4e6a-8ead-6bd9b2d79d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT: New NASA story!\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {i.split(': ')[0]: int(i.split(': ')[1]) for i in response.split(sep='\\n')}\n",
    "if topic_dict['nasa'] == 1:\n",
    "    print(\"ALERT: New NASA story!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402461dc-24d7-4d4a-894c-73f20c5b63cd",
   "metadata": {},
   "source": [
    "# LLMs don't just interpret, they transform\n",
    "E.g., they can translate, check spelling, grammaer, tone, reformat, etc. \n",
    "\n",
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdb342ad-e135-4619-8cec-3adf360db0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, me gustaría pedir una licuadora.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following English text to Spanish:  \n",
    "```Hi, I would like to order a blender```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d084f23-43d6-4e64-9cb5-917be909ed31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language is French. The phrase translates to \"How much does the streetlight cost?\" in English.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Tell me which language this is: \n",
    "```Combien coûte le lampadaire?```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "052a8649-447e-41ac-b556-55ccd2244b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here’s the translation of \"I want to order a basketball\" in French, Spanish, and in English pirate:\n",
      "\n",
      "**French:** Je veux commander un ballon de basket.\n",
      "\n",
      "**Spanish:** Quiero pedir un balón de baloncesto.\n",
      "\n",
      "**English Pirate:** Arrr, I be wantin' to order a basketball, matey!\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following  text to French and Spanish\n",
    "and English pirate: \\\n",
    "```I want to order a basketball```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6402917c-16ab-4c7e-8222-fe7f8889e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In formal Spanish, you would say:  \n",
      "\"¿Le gustaría pedir una almohada?\"\n",
      "\n",
      "In informal Spanish, you would say:  \n",
      "\"¿Te gustaría pedir una almohada?\"\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following text to Spanish in both the \\\n",
    "formal and informal forms: \n",
    "'Would you like to order a pillow?'\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc6f2f0-086a-42a9-9e06-ba445aef1a2d",
   "metadata": {},
   "source": [
    "## Tone translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23062de2-7f82-45de-a941-799d90d61d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Specification for Standing Lamp\n",
      "\n",
      "Dear [Recipient's Name],\n",
      "\n",
      "I hope this message finds you well. \n",
      "\n",
      "I would like to bring to your attention the specifications for the standing lamp. Please take a moment to review the attached document.\n",
      "\n",
      "Thank you for your consideration.\n",
      "\n",
      "Best regards,  \n",
      "Joe\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following from slang to a business letter: \n",
    "'Dude, This is Joe, check out this spec on this standing lamp.'\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f7cbf-d066-402d-b2ec-ac43e0b33622",
   "metadata": {},
   "source": [
    "## Proofreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e71dcd0b-1cf7-41d5-bdd0-767f6f55185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The girl with the black and white puppies has a ball\n",
      "1: No errors found\n",
      "2: It's going to be a long day. Does the car need its oil changed?\n",
      "3: Their goes my freedom. They're going to bring their suitcases.\n",
      "4: You're going to need your notebook\n",
      "5: That medicine affects my ability to sleep. Have you heard of the butterfly effect\n",
      "6: This phrase is to check chatGPT for spelling ability\n"
     ]
    }
   ],
   "source": [
    "text = [ \n",
    "  \"The girl with the black and white puppies have a ball.\",  # The girl has a ball.\n",
    "  \"Yolanda has her notebook.\", # ok\n",
    "  \"Its going to be a long day. Does the car need it’s oil changed?\",  # Homonyms\n",
    "  \"Their goes my freedom. There going to bring they’re suitcases.\",  # Homonyms\n",
    "  \"Your going to need you’re notebook.\",  # Homonyms\n",
    "  \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms\n",
    "  \"This phrase is to cherck chatGPT for speling abilitty\"  # spelling\n",
    "]\n",
    "for ind, t in enumerate(text):\n",
    "    prompt = f\"\"\"Proofread and correct the following text\n",
    "    and rewrite the corrected version. If you don't find\n",
    "    and errors, just say \"No errors found\". Don't use \n",
    "    any punctuation around the text. Remember \"You're means You are\":\n",
    "    ```{t}```\"\"\"\n",
    "    \n",
    "    response = get_completion(prompt)\n",
    "    print(f\"{ind}: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec6a925d-1560-4a9e-9b6a-7d396e51448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a proofread and corrected version of your review:\n",
      "\n",
      "---\n",
      "\n",
      "I got this for my daughter for her birthday because she keeps taking mine from my room. Yes, adults like pandas too! She takes it everywhere with her, and it's super soft and cute. However, one of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. Additionally, it's a bit small for what I paid for it; I think there might be other options that are larger for the same price. On a positive note, it arrived a day earlier than expected, so I got to play with it myself before giving it to my daughter.\n",
      "\n",
      "--- \n",
      "\n",
      "Let me know if you need any further adjustments!\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "Got this for my daughter for her birthday cuz she keeps taking \n",
    "mine from my room.  Yes, adults also like pandas too.  She takes \n",
    "it everywhere with her, and it's super soft and cute.  One of the \n",
    "ears is a bit lower than the other, and I don't think that was\n",
    "designed to be asymmetrical. It's a bit small for what I paid for it \n",
    "though. I think there might be other options that are bigger for\n",
    "the same price.  It arrived a day earlier than expected, so I got\n",
    "to play with it myself before I gave it to my daughter.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"proofread and correct this review: ```{text}```\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "965f128b-873f-4819-85ef-3ae4627a974b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:red;font-weight:700;text-decoration:line-through;'>Got </span><span style='color:green;font-weight:700;'>Here's a proofread and corrected version of your review: </span>\n",
       "\n",
       "<span style='color:green;font-weight:700;'>--- </span>\n",
       "\n",
       "<span style='color:green;font-weight:700;'>I got </span>this for my daughter for her birthday <span style='color:red;font-weight:700;text-decoration:line-through;'>cuz </span><span style='color:green;font-weight:700;'>because </span>she keeps taking <span style='color:red;font-weight:700;text-decoration:line-through;'>¶ </span>mine from my <span style='color:red;font-weight:700;text-decoration:line-through;'>room.  </span><span style='color:green;font-weight:700;'>room. </span>Yes, adults <span style='color:red;font-weight:700;text-decoration:line-through;'>also </span>like pandas <span style='color:red;font-weight:700;text-decoration:line-through;'>too.  </span><span style='color:green;font-weight:700;'>too! </span>She takes <span style='color:red;font-weight:700;text-decoration:line-through;'>¶ </span>it everywhere with her, and it's super soft and <span style='color:red;font-weight:700;text-decoration:line-through;'>cute.  One </span><span style='color:green;font-weight:700;'>cute. However, one </span>of the <span style='color:red;font-weight:700;text-decoration:line-through;'>¶ </span>ears is a bit lower than the other, and I don't think that was <span style='color:red;font-weight:700;text-decoration:line-through;'>¶ </span>designed to be asymmetrical. <span style='color:red;font-weight:700;text-decoration:line-through;'>It's </span><span style='color:green;font-weight:700;'>Additionally, it's </span>a bit small for what I paid for <span style='color:red;font-weight:700;text-decoration:line-through;'>it ¶ though. </span><span style='color:green;font-weight:700;'>it; </span>I think there might be other options that are <span style='color:red;font-weight:700;text-decoration:line-through;'>bigger </span><span style='color:green;font-weight:700;'>larger </span>for <span style='color:red;font-weight:700;text-decoration:line-through;'>¶ </span>the same <span style='color:red;font-weight:700;text-decoration:line-through;'>price.  It </span><span style='color:green;font-weight:700;'>price. On a positive note, it </span>arrived a day earlier than expected, so I got <span style='color:red;font-weight:700;text-decoration:line-through;'>¶ </span>to play with it myself before <span style='color:red;font-weight:700;text-decoration:line-through;'>I gave </span><span style='color:green;font-weight:700;'>giving </span>it to my <span style='color:red;font-weight:700;text-decoration:line-through;'>daughter.</span><span style='color:green;font-weight:700;'>daughter. </span>\n",
       "\n",
       "<span style='color:green;font-weight:700;'>--- </span>\n",
       "\n",
       "<span style='color:green;font-weight:700;'>Let me know if you need any further adjustments!</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from redlines import Redlines\n",
    "\n",
    "diff = Redlines(text,response)\n",
    "display(Markdown(diff.output_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b4fb66-9e55-4ad7-8003-276a0ecd4eca",
   "metadata": {},
   "source": [
    "## Expanding text\n",
    "You can use LLM to brainstorm, or generate lots of text, email.\n",
    "\n",
    "You could use it for evil like generating spam, so ... **please don't**!!\n",
    "\n",
    "If you use an AI customer agent, please indicate it in the email signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "827dbae8-7ec5-4d09-934e-577d71a835f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95f15a-a7a8-4b84-9405-060706715393",
   "metadata": {},
   "source": [
    "## Custom Chatbot\n",
    "Same get_completion() we've been using, but also get_completion() from messages. This lets us send multiple messages in a conversational context. There are multiple roles, we'll go over the three main roles:\n",
    "\n",
    "- `system`: This sets the overall tone/context of the assistant. You typically set this once.\n",
    "- `user`: represents what the person talking to chatgpt is saing (what is the question, request, command coming in).\n",
    "- `assistant`: represents response generated by the AI (could be code snippet, explanations, etc).\n",
    "\n",
    "Using these (especially user/assistant), you can represenet an ongoing *dialog* with an chatbot/agent, so it has ongoing context for a conversation. Wehn you use chatgpt, you are the user, the assistant is the chatgpt response.\n",
    "\n",
    "So you can feed multiple messages with alternating roles into `chat.completions.create()` to get a response as a dictionary:\n",
    "\n",
    "    messages =  [  \n",
    "    {'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    \n",
    "    {'role':'user', 'content':'tell me a joke'},   \n",
    "    {'role':'assistant', 'content':'Why did the chicken cross the road'},   \n",
    "    {'role':'user', 'content':'I don\\'t know'}  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5df273-81d0-4c2c-92de-906942f507e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "202305c8-a1a0-4273-8774-3f26361765e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0):\n",
    "    \"\"\"\n",
    "    Adapted from DeepLearning.AI's prompt engineering class: https://learn.deeplearning.ai/\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=1000,\n",
    "        temperature=temperature  # 0 for consistency\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7f0539a-abfe-4169-87e8-0ef8e9fa2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    \n",
    "{'role':'user', 'content':'tell me a joke'},   \n",
    "{'role':'assistant', 'content':'Why did the chicken cross the road'},   \n",
    "{'role':'user', 'content':'I don\\'t know'}  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2c21936-c5e5-4750-8801-530fc640526f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get to the other side, good sir! Oh, what a jest!\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_from_messages(messages, temperature=0.5)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8086134-2441-4826-9b87-2dbe3b34c97a",
   "metadata": {},
   "source": [
    "Limitations: \n",
    "- there is no memory of conversation unless you build it in. It would be nice to have a memory with your LLM. Commercial LLMs (like ChatGPT) add this feature so they will (for instance) add to their long-term memory if you have kids or what your interests and hobbies are. \n",
    "- If you have data specific to your org, it would be nice to ingest it and have it available. \n",
    "\n",
    "Handling memory buffers, accessing org-specific data require more than just access to a language model. There are lots of tools that have sprung up for this. E.g., Langchain, huggingface, provide tooling for pipelines to integrate LLMs into larger-scale applications and include tools for memory buffers, retrieveal-augmented generation of content (RAG). \n",
    "\n",
    "Both langchain and hugging face are powerful tools used in industry so it will be very useful to know how to use them (and they are very often used together). \n",
    "\n",
    "Off we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4fb1e-aa7f-4cd6-9504-94e91ec35aea",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- create interactive chat that takes in previous pairs of interactions.\n",
    "- Create dash application (or voila) that serves this as a web page. \n",
    "the sky is the limit with this stuff.\n",
    "\n",
    "- this is a great place to be creative and come up with fun chatbot idea project for students to build and play with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d6696-8e5b-4d2d-9190-4b61965d2d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
